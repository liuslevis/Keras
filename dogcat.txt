Using TensorFlow backend.Loading image 0 of 2000Loading image 250 of 2000Loading image 500 of 2000Loading image 750 of 2000Loading image 1000 of 2000Loading image 1250 of 2000Loading image 1500 of 2000Loading image 1750 of 2000Loading image 0 of 1600Loading image 250 of 1600Loading image 500 of 1600Loading image 750 of 1600Loading image 1000 of 1600Loading image 1250 of 1600Loading image 1500 of 1600Train shape: (2000, 150, 150, 3)Valid shape: (1600, 150, 150, 3)dogcat.py:82: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, kernel_size=(3, 3), padding="same", input_shape=(150, 150,...)`  model.add(Conv2D(32, kernel_size=(3, 3), border_mode='same', input_shape=input_shape))dogcat.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, kernel_size=(3, 3), padding="same")`  model.add(Conv2D(32, kernel_size=(3, 3), border_mode='same'))dogcat.py:90: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, kernel_size=(3, 3), padding="same")`  model.add(Conv2D(64, kernel_size=(3, 3), border_mode='same'))_________________________________________________________________Layer (type)                 Output Shape              Param #=================================================================conv2d_1 (Conv2D)            (None, 150, 150, 32)      896_________________________________________________________________activation_1 (Activation)    (None, 150, 150, 32)      0_________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 75, 75, 32)        0_________________________________________________________________conv2d_2 (Conv2D)            (None, 75, 75, 32)        9248_________________________________________________________________activation_2 (Activation)    (None, 75, 75, 32)        0_________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 37, 37, 32)        0_________________________________________________________________conv2d_3 (Conv2D)            (None, 37, 37, 64)        18496_________________________________________________________________activation_3 (Activation)    (None, 37, 37, 64)        0_________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 18, 18, 64)        0_________________________________________________________________flatten_1 (Flatten)          (None, 20736)             0_________________________________________________________________dense_1 (Dense)              (None, 64)                1327168_________________________________________________________________activation_4 (Activation)    (None, 64)                0_________________________________________________________________dropout_1 (Dropout)          (None, 64)                0_________________________________________________________________dense_2 (Dense)              (None, 1)                 65_________________________________________________________________activation_5 (Activation)    (None, 1)                 0=================================================================Total params: 1,355,873Trainable params: 1,355,873Non-trainable params: 0_________________________________________________________________Train on 2000 samples, validate on 1600 samplesEpoch 1/100W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.2000/2000 [==============================] - 103s - loss: 1.7053 - acc: 0.5025 - val_loss: 0.7026 - val_acc: 0.5125Epoch 2/1002000/2000 [==============================] - 101s - loss: 0.7171 - acc: 0.5200 - val_loss: 0.6951 - val_acc: 0.5225Epoch 3/1002000/2000 [==============================] - 102s - loss: 0.7008 - acc: 0.5355 - val_loss: 0.6913 - val_acc: 0.5425Epoch 4/1002000/2000 [==============================] - 101s - loss: 0.7000 - acc: 0.5490 - val_loss: 0.6924 - val_acc: 0.5481Epoch 5/1002000/2000 [==============================] - 104s - loss: 0.6856 - acc: 0.5405 - val_loss: 0.6914 - val_acc: 0.5325Epoch 6/1002000/2000 [==============================] - 105s - loss: 0.6894 - acc: 0.5555 - val_loss: 0.7046 - val_acc: 0.5306Epoch 7/1002000/2000 [==============================] - 103s - loss: 0.6837 - acc: 0.5540 - val_loss: 0.6932 - val_acc: 0.5506Epoch 8/1002000/2000 [==============================] - 101s - loss: 0.6806 - acc: 0.5715 - val_loss: 0.6939 - val_acc: 0.5531Epoch 9/1002000/2000 [==============================] - 100s - loss: 0.6741 - acc: 0.5750 - val_loss: 0.7217 - val_acc: 0.5256Epoch 10/1002000/2000 [==============================] - 104s - loss: 0.6765 - acc: 0.5770 - val_loss: 0.6995 - val_acc: 0.5262Epoch 11/1002000/2000 [==============================] - 104s - loss: 0.6775 - acc: 0.5840 - val_loss: 0.6898 - val_acc: 0.5694Epoch 12/1002000/2000 [==============================] - 102s - loss: 0.6674 - acc: 0.5910 - val_loss: 0.6938 - val_acc: 0.5406Epoch 13/1002000/2000 [==============================] - 101s - loss: 0.6696 - acc: 0.5890 - val_loss: 0.6948 - val_acc: 0.5731Epoch 14/1002000/2000 [==============================] - 100s - loss: 0.6489 - acc: 0.6155 - val_loss: 0.6934 - val_acc: 0.5569Epoch 15/1002000/2000 [==============================] - 100s - loss: 0.6433 - acc: 0.6130 - val_loss: 0.6882 - val_acc: 0.5637Epoch 16/1002000/2000 [==============================] - 100s - loss: 0.6338 - acc: 0.6245 - val_loss: 0.6839 - val_acc: 0.5744Epoch 17/1002000/2000 [==============================] - 104s - loss: 0.6327 - acc: 0.6355 - val_loss: 0.7053 - val_acc: 0.5981Epoch 18/1002000/2000 [==============================] - 106s - loss: 0.6175 - acc: 0.6310 - val_loss: 0.6767 - val_acc: 0.5913Epoch 19/1002000/2000 [==============================] - 102s - loss: 0.6116 - acc: 0.6480 - val_loss: 0.6813 - val_acc: 0.5913Epoch 20/1002000/2000 [==============================] - 102s - loss: 0.6121 - acc: 0.6575 - val_loss: 0.6659 - val_acc: 0.6394Epoch 21/1002000/2000 [==============================] - 121s - loss: 0.5981 - acc: 0.6640 - val_loss: 0.7300 - val_acc: 0.6069Epoch 22/1002000/2000 [==============================] - 116s - loss: 0.5859 - acc: 0.6720 - val_loss: 0.6687 - val_acc: 0.6250Epoch 23/1002000/2000 [==============================] - 126s - loss: 0.5793 - acc: 0.6930 - val_loss: 0.7087 - val_acc: 0.6106Epoch 24/1002000/2000 [==============================] - 129s - loss: 0.5618 - acc: 0.6940 - val_loss: 0.6886 - val_acc: 0.6288Epoch 25/1002000/2000 [==============================] - 122s - loss: 0.5570 - acc: 0.6995 - val_loss: 0.6693 - val_acc: 0.6344Epoch 26/1002000/2000 [==============================] - 124s - loss: 0.5437 - acc: 0.7035 - val_loss: 0.6786 - val_acc: 0.6506Epoch 27/1002000/2000 [==============================] - 123s - loss: 0.5229 - acc: 0.7135 - val_loss: 0.6716 - val_acc: 0.6506Epoch 28/1002000/2000 [==============================] - 129s - loss: 0.5108 - acc: 0.7345 - val_loss: 0.7077 - val_acc: 0.6356Epoch 29/1002000/2000 [==============================] - 129s - loss: 0.5098 - acc: 0.7305 - val_loss: 0.7283 - val_acc: 0.6031Epoch 30/1002000/2000 [==============================] - 111s - loss: 0.4908 - acc: 0.7475 - val_loss: 0.6870 - val_acc: 0.6475Epoch 31/1002000/2000 [==============================] - 103s - loss: 0.4739 - acc: 0.7540 - val_loss: 0.6531 - val_acc: 0.6594Epoch 32/1002000/2000 [==============================] - 100s - loss: 0.4649 - acc: 0.7610 - val_loss: 0.6882 - val_acc: 0.6625Epoch 33/1002000/2000 [==============================] - 100s - loss: 0.4542 - acc: 0.7760 - val_loss: 0.6793 - val_acc: 0.6475Epoch 34/1002000/2000 [==============================] - 102s - loss: 0.4355 - acc: 0.7880 - val_loss: 0.6586 - val_acc: 0.6731Epoch 35/1002000/2000 [==============================] - 106s - loss: 0.4275 - acc: 0.7930 - val_loss: 0.6467 - val_acc: 0.6763Epoch 36/1002000/2000 [==============================] - 100s - loss: 0.4058 - acc: 0.7990 - val_loss: 0.6737 - val_acc: 0.6575Epoch 37/1002000/2000 [==============================] - 103s - loss: 0.3957 - acc: 0.8105 - val_loss: 0.6404 - val_acc: 0.6744Epoch 38/1002000/2000 [==============================] - 130s - loss: 0.3867 - acc: 0.8130 - val_loss: 0.6575 - val_acc: 0.6844Epoch 39/1002000/2000 [==============================] - 126s - loss: 0.3590 - acc: 0.8350 - val_loss: 0.6731 - val_acc: 0.6794Epoch 40/1002000/2000 [==============================] - 131s - loss: 0.3423 - acc: 0.8490 - val_loss: 0.6501 - val_acc: 0.6894Epoch 41/1002000/2000 [==============================] - 130s - loss: 0.3329 - acc: 0.8455 - val_loss: 0.6655 - val_acc: 0.6837Epoch 42/1002000/2000 [==============================] - 128s - loss: 0.3327 - acc: 0.8455 - val_loss: 0.7067 - val_acc: 0.6831Epoch 43/1002000/2000 [==============================] - 133s - loss: 0.3192 - acc: 0.8490 - val_loss: 0.7335 - val_acc: 0.6719Epoch 44/1002000/2000 [==============================] - 132s - loss: 0.2952 - acc: 0.8700 - val_loss: 0.7842 - val_acc: 0.6744Epoch 45/1002000/2000 [==============================] - 128s - loss: 0.2965 - acc: 0.8770 - val_loss: 0.7439 - val_acc: 0.6656Epoch 46/1002000/2000 [==============================] - 106s - loss: 0.2805 - acc: 0.8795 - val_loss: 0.7720 - val_acc: 0.6719Epoch 47/1002000/2000 [==============================] - 106s - loss: 0.2706 - acc: 0.8775 - val_loss: 0.6881 - val_acc: 0.6825Epoch 48/1002000/2000 [==============================] - 99s - loss: 0.2684 - acc: 0.8845 - val_loss: 0.7642 - val_acc: 0.6875Epoch 49/1002000/2000 [==============================] - 99s - loss: 0.2422 - acc: 0.8955 - val_loss: 0.7523 - val_acc: 0.6794Epoch 50/1002000/2000 [==============================] - 99s - loss: 0.2398 - acc: 0.9035 - val_loss: 0.7376 - val_acc: 0.6831Epoch 51/1002000/2000 [==============================] - 99s - loss: 0.2378 - acc: 0.8970 - val_loss: 0.7724 - val_acc: 0.6831Epoch 52/1002000/2000 [==============================] - 99s - loss: 0.2268 - acc: 0.9055 - val_loss: 0.8114 - val_acc: 0.6713Epoch 53/1002000/2000 [==============================] - 66s - loss: 0.2090 - acc: 0.9115 - val_loss: 0.7758 - val_acc: 0.6931Epoch 54/1002000/2000 [==============================] - 70s - loss: 0.2106 - acc: 0.9135 - val_loss: 0.8096 - val_acc: 0.6900Epoch 55/1002000/2000 [==============================] - 68s - loss: 0.1994 - acc: 0.9215 - val_loss: 0.7724 - val_acc: 0.6919Epoch 56/1002000/2000 [==============================] - 66s - loss: 0.1881 - acc: 0.9255 - val_loss: 0.8167 - val_acc: 0.6925Epoch 57/1002000/2000 [==============================] - 69s - loss: 0.1852 - acc: 0.9290 - val_loss: 0.8561 - val_acc: 0.6881Epoch 58/1002000/2000 [==============================] - 66s - loss: 0.1870 - acc: 0.9240 - val_loss: 0.8263 - val_acc: 0.6956Epoch 59/1002000/2000 [==============================] - 66s - loss: 0.1747 - acc: 0.9265 - val_loss: 0.8289 - val_acc: 0.6906Epoch 60/1002000/2000 [==============================] - 68s - loss: 0.1719 - acc: 0.9380 - val_loss: 0.8814 - val_acc: 0.7000Epoch 61/1002000/2000 [==============================] - 66s - loss: 0.1714 - acc: 0.9365 - val_loss: 0.8822 - val_acc: 0.6975Epoch 62/1002000/2000 [==============================] - 66s - loss: 0.1470 - acc: 0.9465 - val_loss: 0.9635 - val_acc: 0.6963Epoch 63/1002000/2000 [==============================] - 66s - loss: 0.1484 - acc: 0.9465 - val_loss: 0.8674 - val_acc: 0.6919Epoch 64/1002000/2000 [==============================] - 66s - loss: 0.1525 - acc: 0.9405 - val_loss: 0.9146 - val_acc: 0.6887Epoch 65/1002000/2000 [==============================] - 66s - loss: 0.1453 - acc: 0.9475 - val_loss: 0.8893 - val_acc: 0.6969Epoch 66/1002000/2000 [==============================] - 66s - loss: 0.1398 - acc: 0.9505 - val_loss: 0.9657 - val_acc: 0.6900Epoch 67/1002000/2000 [==============================] - 66s - loss: 0.1346 - acc: 0.9490 - val_loss: 0.9163 - val_acc: 0.6937Epoch 68/1002000/2000 [==============================] - 66s - loss: 0.1260 - acc: 0.9555 - val_loss: 0.9391 - val_acc: 0.6944Epoch 69/1002000/2000 [==============================] - 66s - loss: 0.1257 - acc: 0.9545 - val_loss: 0.8823 - val_acc: 0.7006Epoch 70/1002000/2000 [==============================] - 66s - loss: 0.1211 - acc: 0.9565 - val_loss: 0.9741 - val_acc: 0.6925Epoch 71/1002000/2000 [==============================] - 66s - loss: 0.1075 - acc: 0.9585 - val_loss: 1.0166 - val_acc: 0.6956Epoch 72/1002000/2000 [==============================] - 66s - loss: 0.1113 - acc: 0.9600 - val_loss: 1.0798 - val_acc: 0.6750Epoch 73/1002000/2000 [==============================] - 65s - loss: 0.1171 - acc: 0.9560 - val_loss: 0.9077 - val_acc: 0.6925Epoch 74/1002000/2000 [==============================] - 66s - loss: 0.1060 - acc: 0.9615 - val_loss: 1.0195 - val_acc: 0.6887Epoch 75/1002000/2000 [==============================] - 66s - loss: 0.1047 - acc: 0.9610 - val_loss: 1.0786 - val_acc: 0.6925Epoch 76/1002000/2000 [==============================] - 66s - loss: 0.1013 - acc: 0.9630 - val_loss: 1.0695 - val_acc: 0.7025Epoch 77/1002000/2000 [==============================] - 66s - loss: 0.1081 - acc: 0.9620 - val_loss: 1.0051 - val_acc: 0.6963Epoch 78/1002000/2000 [==============================] - 66s - loss: 0.0979 - acc: 0.9645 - val_loss: 0.9743 - val_acc: 0.7006Epoch 79/1002000/2000 [==============================] - 66s - loss: 0.0964 - acc: 0.9685 - val_loss: 1.0754 - val_acc: 0.6875Epoch 80/1002000/2000 [==============================] - 66s - loss: 0.0933 - acc: 0.9675 - val_loss: 1.0298 - val_acc: 0.6963Epoch 81/1002000/2000 [==============================] - 66s - loss: 0.0929 - acc: 0.9680 - val_loss: 1.0391 - val_acc: 0.6956Epoch 82/1002000/2000 [==============================] - 65s - loss: 0.0832 - acc: 0.9700 - val_loss: 1.1199 - val_acc: 0.6869Epoch 83/1002000/2000 [==============================] - 66s - loss: 0.0910 - acc: 0.9655 - val_loss: 1.0272 - val_acc: 0.7013Epoch 84/1002000/2000 [==============================] - 66s - loss: 0.0927 - acc: 0.9670 - val_loss: 0.9800 - val_acc: 0.6963Epoch 85/1002000/2000 [==============================] - 66s - loss: 0.0788 - acc: 0.9725 - val_loss: 1.0170 - val_acc: 0.7044Epoch 86/1002000/2000 [==============================] - 66s - loss: 0.0787 - acc: 0.9725 - val_loss: 1.1236 - val_acc: 0.6950Epoch 87/1002000/2000 [==============================] - 66s - loss: 0.0855 - acc: 0.9695 - val_loss: 1.0353 - val_acc: 0.7094Epoch 88/1002000/2000 [==============================] - 66s - loss: 0.0833 - acc: 0.9695 - val_loss: 1.1107 - val_acc: 0.7075Epoch 89/1002000/2000 [==============================] - 66s - loss: 0.0856 - acc: 0.9670 - val_loss: 1.0803 - val_acc: 0.6994Epoch 90/1002000/2000 [==============================] - 66s - loss: 0.0802 - acc: 0.9730 - val_loss: 1.1602 - val_acc: 0.7169Epoch 91/1002000/2000 [==============================] - 66s - loss: 0.0624 - acc: 0.9820 - val_loss: 1.0913 - val_acc: 0.7113Epoch 92/1002000/2000 [==============================] - 66s - loss: 0.0796 - acc: 0.9750 - val_loss: 1.1126 - val_acc: 0.7069Epoch 93/1002000/2000 [==============================] - 67s - loss: 0.0764 - acc: 0.9755 - val_loss: 1.1479 - val_acc: 0.7000Epoch 94/1002000/2000 [==============================] - 66s - loss: 0.0655 - acc: 0.9785 - val_loss: 1.1055 - val_acc: 0.7063Epoch 95/1002000/2000 [==============================] - 66s - loss: 0.0763 - acc: 0.9760 - val_loss: 1.0906 - val_acc: 0.7069Epoch 96/1002000/2000 [==============================] - 66s - loss: 0.0718 - acc: 0.9740 - val_loss: 1.1961 - val_acc: 0.6875Epoch 97/1002000/2000 [==============================] - 66s - loss: 0.0656 - acc: 0.9760 - val_loss: 1.1775 - val_acc: 0.6919Epoch 98/1002000/2000 [==============================] - 66s - loss: 0.0625 - acc: 0.9775 - val_loss: 1.2445 - val_acc: 0.7031Epoch 99/1002000/2000 [==============================] - 66s - loss: 0.0577 - acc: 0.9810 - val_loss: 1.1955 - val_acc: 0.6975Epoch 100/1002000/2000 [==============================] - 66s - loss: 0.0641 - acc: 0.9810 - val_loss: 1.2318 - val_acc: 0.7044valid loss: 1.23184832573valid accuracy: 0.704375